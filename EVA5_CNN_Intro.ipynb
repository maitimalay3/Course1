{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of EVA5: CNN Intro",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iCzeYKWQFB2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXh_oyP1QFqS"
      },
      "source": [
        "1. What are Channels and Kernels (according to EVA)?\n",
        "Ans: Karnel is a filter which helps extract certain features, it’s a feature extractor. Features are spatial pattern, spatial frequency, which is the building block of an image.\n",
        " Running the karnel over an image creates a channel. Channel is a feature map. Size of the channel is the size of the image.\n",
        "\n",
        "2. Why should we (nearly) always use 3x3 kernels?\n",
        "Ans: The reasons behind 3 *3 karnels \n",
        "a.\tThe number of parameters/weights to calculate are small compared to any large filter size such as 5*5 or 11*11\n",
        "b.\t Any other large filters can be replicated with 3*3 with reduced parameter size, hence less computational time\n",
        "c.\tFilter size is small, computationally first\n",
        "d.\tNVIDIA accelerated 3*3 use \n",
        "e.\t3* 3 is more efficient and flexible to get different complicated angles and edges. It has more flexibility compared to 2*2. \n",
        "f.\t4*4 doesn’t have the line of symmetry, but 3*3 has.\n",
        "\n",
        "\n",
        "3. How many times to we need to perform 3x3 convolutions operations to reach close to 1x1 from 199x199 (type each layer output like 199x199 > 197x197...)\n",
        "\n",
        "Ans: 100\n",
        "199\t*\t199\n",
        "197\t*\t197\n",
        "195\t*\t195\n",
        "193\t*\t193\n",
        "191\t*\t191\n",
        "189\t*\t189\n",
        "187\t*\t187\n",
        "185\t*\t185\n",
        "183\t*\t183\n",
        "181\t*\t181\n",
        "179\t*\t179\n",
        "177\t*\t177\n",
        "175\t*\t175\n",
        "173\t*\t173\n",
        "171\t*\t171\n",
        "169\t*\t169\n",
        "167\t*\t167\n",
        "165\t*\t165\n",
        "163\t*\t163\n",
        "161\t*\t161\n",
        "159\t*\t159\n",
        "157\t*\t157\n",
        "155\t*\t155\n",
        "153\t*\t153\n",
        "151\t*\t151\n",
        "149\t*\t149\n",
        "147\t*\t147\n",
        "145\t*\t145\n",
        "143\t*\t143\n",
        "141\t*\t141\n",
        "139\t*\t139\n",
        "137\t*\t137\n",
        "135\t*\t135\n",
        "133\t*\t133\n",
        "131\t*\t131\n",
        "129\t*\t129\n",
        "127\t*\t127\n",
        "125\t*\t125\n",
        "123\t*\t123\n",
        "121\t*\t121\n",
        "119\t*\t119\n",
        "117\t*\t117\n",
        "115\t*\t115\n",
        "113\t*\t113\n",
        "111\t*\t111\n",
        "109\t*\t109\n",
        "107\t*\t107\n",
        "105\t*\t105\n",
        "103\t*\t103\n",
        "101\t*\t101\n",
        "99\t*\t99\n",
        "97\t*\t97\n",
        "95\t*\t95\n",
        "93\t*\t93\n",
        "91\t*\t91\n",
        "89\t*\t89\n",
        "87\t*\t87\n",
        "85\t*\t85\n",
        "83\t*\t83\n",
        "81\t*\t81\n",
        "79\t*\t79\n",
        "77\t*\t77\n",
        "75\t*\t75\n",
        "73\t*\t73\n",
        "71\t*\t71\n",
        "69\t*\t69\n",
        "67\t*\t67\n",
        "65\t*\t65\n",
        "63\t*\t63\n",
        "61\t*\t61\n",
        "59\t*\t59\n",
        "57\t*\t57\n",
        "55\t*\t55\n",
        "53\t*\t53\n",
        "51\t*\t51\n",
        "49\t*\t49\n",
        "47\t*\t47\n",
        "45\t*\t45\n",
        "43\t*\t43\n",
        "41\t*\t41\n",
        "39\t*\t39\n",
        "37\t*\t37\n",
        "35\t*\t35\n",
        "33\t*\t33\n",
        "31\t*\t31\n",
        "29\t*\t29\n",
        "27\t*\t27\n",
        "25\t*\t25\n",
        "23\t*\t23\n",
        "21\t*\t21\n",
        "19\t*\t19\n",
        "17\t*\t17\n",
        "15\t*\t15\n",
        "13\t*\t13\n",
        "11\t*\t11\n",
        "9\t*\t9\n",
        "7\t*\t7\n",
        "5\t*\t5\n",
        "3\t*\t3\n",
        "1\t*\t1\n",
        "\n",
        "\n",
        "4.How are kernels initialized? \n",
        "Ans: 1.\tKernels are initialized randomly because\n",
        "a.\tThe convergence of the parameters which needs to be optimized to get the lowest cost functions travels fast to reach the optimal values if it initiated randomly compared to a single vale.\n",
        "b.\tBreaking the symmetry: Given a set of inputs, if the weights of the hidden units of any given layer gets initialized with the same value, the travel path for optimization of the weights for the next iteration will be indecisive and less efficient compared to random values. The random values increase the chances of Avoids the chances relative weights to move to direction to reach the final destiny (minimum value of the cost function).\n",
        "c.\tRandom initialization also reduces the chance of getting stuck in local minima.\n",
        "\n",
        "5. What happens during the training of a DNN?\n",
        "Ans: \n",
        "a.\tStart with values (often random) for the network parameters (wij weights and bj biases).\n",
        "b.\tTake the input data and pass them through the network to obtain their prediction.\n",
        "c.\tCompare these predictions obtained with the values of expected labels and calculate the loss with them.\n",
        "d.\tPerform the backpropagation in order to propagate this loss to each and every one of the weights that make up the model of the neural network.\n",
        "e.\tUse this propagated information to update the parameters of the neural network with the gradient descent in a way that the total loss is reduced and a better model is obtained.\n",
        "f.\tContinue iterating in the previous steps until we consider that we have a good model.\n",
        "8.\tVanishing gradient problem is one of the key issue in DNN, there many fixes and workarounds have been proposed and investigated, such as alternate weight initialization schemes, unsupervised pre-training, layer-wise training, and variations on gradient descent. Perhaps the most common change is the use of the rectified linear activation function that has become the new default, instead of the hyperbolic tangent activation function\n",
        "\n"
      ]
    }
  ]
}