{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D96Kbaze4D8n"
      },
      "source": [
        "# Reading and Transforming the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57R4-uv6CfAU"
      },
      "source": [
        "import torch\r\n",
        "import torchvision # provide access to datasets, models, transforms, utils, etc\r\n",
        "import torchvision.transforms as transforms\r\n",
        "\r\n",
        "train_set = torchvision.datasets.EMNIST(\r\n",
        "    root='./data'\r\n",
        "    ,train=True\r\n",
        "    ,download=True\r\n",
        "    ,split = \"balanced\"\r\n",
        "    ,transform=transforms.Compose([\r\n",
        "        transforms.ToTensor()\r\n",
        "    ])\r\n",
        ")"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFr3r2TQ4MFm"
      },
      "source": [
        "# Checking the dataset and distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "naOAyVioFIvm",
        "outputId": "c104aa30-fe7e-4ad4-8f7b-9dbc659d1252"
      },
      "source": [
        "print(len(train_set))\r\n",
        "train_set.train_labels.bincount()"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "112800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:48: UserWarning: train_labels has been renamed targets\n",
            "  warnings.warn(\"train_labels has been renamed targets\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2400, 2400, 2400, 2400, 2400, 2400, 2400, 2400, 2400, 2400, 2400, 2400,\n",
              "        2400, 2400, 2400, 2400, 2400, 2400, 2400, 2400, 2400, 2400, 2400, 2400,\n",
              "        2400, 2400, 2400, 2400, 2400, 2400, 2400, 2400, 2400, 2400, 2400, 2400,\n",
              "        2400, 2400, 2400, 2400, 2400, 2400, 2400, 2400, 2400, 2400, 2400])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-i4BwgPF4XkA"
      },
      "source": [
        "# Check a single image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "RJmjPk-aC0WJ",
        "outputId": "8e32e20d-88be-4e5b-84d0-2fdbde94e242"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "sample = next(iter(train_set))\r\n",
        "len(sample)\r\n",
        "image, label = sample\r\n",
        "plt.imshow(image.squeeze(), cmap='gray')\r\n",
        "print('label:', label)\r\n",
        "image.shape, label"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "label: 45\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:48: UserWarning: train_labels has been renamed targets\n",
            "  warnings.warn(\"train_labels has been renamed targets\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 28, 28]), 45)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ou8FoSvH4dzQ"
      },
      "source": [
        "# CNN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvwnrlbLDek5"
      },
      "source": [
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "class Network(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super().__init__()\r\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=10, kernel_size=3, padding = 1) #28*28* 1 > 28* 28 10 # There will be max pool after conv1. After Max pool the recepetive field will be 6*6\r\n",
        "        self.conv2 = nn.Conv2d(in_channels=10, out_channels= 10 , kernel_size=3, padding = 1) #14*14 *10 > 14 *14 10 # There will be max pool after conv2. After Max pool the recepetive field will be 16 *16\r\n",
        "        self.conv3 = nn.Conv2d(in_channels=10, out_channels= 20 , kernel_size=3) # 7 * 7 * 10 >  5 * 5 * 20 # Cov3 Receptive field 18\r\n",
        "        self.conv4 = nn.Conv2d(in_channels= 20, out_channels= 20 , kernel_size=3) # 5 *5 *20 > 3* 3* 20 # Cov4 Receptive field 20\r\n",
        "        self.conv5 = nn.Conv2d(in_channels= 20, out_channels= 30 , kernel_size=3) # 3* 3* 20 > 1 * 1 * 30 # Cov5 Receptive field 22\r\n",
        "        self.conv6 = nn.Conv2d(in_channels= 30, out_channels= 47 , kernel_size=1) # 3* 3* 20 > 1 * 1 * 30\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "    def forward(self, t):\r\n",
        "      # (1) input layer\r\n",
        "      t = t\r\n",
        "       # (2) hidden conv layer\r\n",
        "      t = self.conv1(t)\r\n",
        "      t = F.relu(t)\r\n",
        "      t = F.max_pool2d(t, kernel_size=2)\r\n",
        "\r\n",
        "        # (3) hidden conv layer\r\n",
        "      t = self.conv2(t)\r\n",
        "      t = F.relu(t)\r\n",
        "      t = F.max_pool2d(t, kernel_size=2)\r\n",
        "       \r\n",
        "      # (4) hidden conv layer\r\n",
        "      t = self.conv3(t)\r\n",
        "      t = F.relu(t)\r\n",
        "       # (5) hidden conv layer  \r\n",
        "      t = self.conv4(t)\r\n",
        "      t = F.relu(t)\r\n",
        "      # (6) hidden conv layer  \r\n",
        "      t = self.conv5(t)\r\n",
        "      t = F.relu(t)\r\n",
        "      # (6) hidden conv layer ; Streching the image to 47 channels  \r\n",
        "      t = self.conv6(t)\r\n",
        "      #make it 1d\r\n",
        "      t = t.view(-1, 47)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "      return F.log_softmax(t)\r\n",
        "      "
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-0UGZID615W"
      },
      "source": [
        "# Function to check the prediction and actual"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sF3GOxtsvdNv"
      },
      "source": [
        "def get_num_correct(preds, labels):\r\n",
        "  return preds.argmax(dim=1).eq(labels).sum().item()"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lqrl6dLX7MOD"
      },
      "source": [
        "#Initiate the model class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsHJUXFU7P2B"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-2s0IHiPDlo",
        "outputId": "06b426e8-f5a9-4731-bee2-fb67a13c555e"
      },
      "source": [
        "network = Network()\r\n",
        "print(network)\r\n"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Network(\n",
            "  (conv1): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv3): Conv2d(10, 20, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (conv4): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (conv5): Conv2d(20, 30, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (conv6): Conv2d(30, 47, kernel_size=(1, 1), stride=(1, 1))\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvJCtTGk7Xey"
      },
      "source": [
        "# Check a single image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WhNQK01PVZE",
        "outputId": "4289fc9f-17b5-495e-e0ed-8e389c3ec1d7"
      },
      "source": [
        "torch.set_grad_enabled(False)\r\n",
        "sample = next(iter(train_set)) \r\n",
        "image, label = sample\r\n",
        "image.shape, image.unsqueeze(0).shape\r\n",
        "pred = network(image.unsqueeze(0))\r\n",
        "pred.shape"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.autograd.grad_mode.set_grad_enabled at 0x7fad30cdfac8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAdwz9Aa7enE"
      },
      "source": [
        "# Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPxfiDlTt5-T",
        "outputId": "6e4b1784-1fea-4298-a145-0b588ab85781"
      },
      "source": [
        "import torch.optim as optim\r\n",
        "\r\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=100)\r\n",
        "optimizer = optim.Adam(network.parameters(), lr=0.01)\r\n",
        "torch.set_grad_enabled(True)\r\n",
        "\r\n",
        "for epoch in range(20):\r\n",
        "\r\n",
        "    total_loss = 0\r\n",
        "    total_correct = 0\r\n",
        "\r\n",
        "    for batch in train_loader: # Get Batch\r\n",
        "        images, labels = batch \r\n",
        "\r\n",
        "        preds = network(images) # Pass Batch\r\n",
        "        loss = F.cross_entropy(preds, labels) # Calculate Loss\r\n",
        "\r\n",
        "        optimizer.zero_grad()\r\n",
        "        loss.backward() # Calculate Gradients\r\n",
        "        optimizer.step() # Update Weights\r\n",
        "\r\n",
        "        total_loss += loss.item()\r\n",
        "        total_correct += get_num_correct(preds, labels)\r\n",
        "\r\n",
        "    print(\r\n",
        "        \"epoch\", epoch, \r\n",
        "        \"total_correct:\", total_correct, \r\n",
        "        \"loss:\", total_loss\r\n",
        "    )"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:46: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 0 total_correct: 64896 loss: 1563.7681303620338\n",
            "epoch 1 total_correct: 81388 loss: 975.7954006195068\n",
            "epoch 2 total_correct: 83347 loss: 910.7536614835262\n",
            "epoch 3 total_correct: 84506 loss: 876.2178189456463\n",
            "epoch 4 total_correct: 85027 loss: 859.2314097285271\n",
            "epoch 5 total_correct: 85209 loss: 849.9899033606052\n",
            "epoch 6 total_correct: 85491 loss: 838.4625252783298\n",
            "epoch 7 total_correct: 85635 loss: 832.7492441833019\n",
            "epoch 8 total_correct: 85715 loss: 828.8040691316128\n",
            "epoch 9 total_correct: 85957 loss: 825.1647506356239\n",
            "epoch 10 total_correct: 85908 loss: 821.4768992364407\n",
            "epoch 11 total_correct: 85877 loss: 822.9937447607517\n",
            "epoch 12 total_correct: 86229 loss: 813.6485145688057\n",
            "epoch 13 total_correct: 86443 loss: 804.8673409819603\n",
            "epoch 14 total_correct: 86487 loss: 801.6357666254044\n",
            "epoch 15 total_correct: 86683 loss: 793.6590873599052\n",
            "epoch 16 total_correct: 86618 loss: 794.7180162966251\n",
            "epoch 17 total_correct: 86673 loss: 792.8923965990543\n",
            "epoch 18 total_correct: 87037 loss: 783.9636635482311\n",
            "epoch 19 total_correct: 87083 loss: 780.6712603271008\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhBd-yLuu-_q"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}